{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('shakespere.txt', 'r').read()\n",
    "text = text[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars:  53\n",
      "number of sequences: 1654\n"
     ]
    }
   ],
   "source": [
    "# Getting sorted listof char in text\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars: ', len(chars))\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# Spitting text into sentences\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('number of sequences:', len(sentences))\n",
    "\n",
    "# Character level One - hot Encoding\n",
    "x_train = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y_train = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_train[i, t, char_indices[char]] = 1\n",
    "    y_train[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Next Batch Generator\n",
    "def next_batch(x, y, batch_size):\n",
    "    N = x.shape[0]\n",
    "    batch_indices = np.random.permutation(N)[:batch_size]\n",
    "    x_batch = x[batch_indices]\n",
    "    y_batch = y[batch_indices]\n",
    "    return x_batch, y_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Dimensions\n",
    "input_dim = len(chars)       # input dimension\n",
    "seq_max_len = maxlen         # sequence maximum length\n",
    "out_dim = len(chars)         # output dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters\n",
    "learning_rate = 0.01    # The optimization initial learning rate\n",
    "training_steps = 1000  # Total number of training steps\n",
    "batch_size = 64        # batch size\n",
    "display_freq = 100    # Frequency of displaying the training results\n",
    "num_hidden_units = 64   # number of hidden units "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "    return tf.get_variable('W',\n",
    "                           dtype=tf.float32,\n",
    "                           shape=shape,\n",
    "                           initializer=initer)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n",
    "    return tf.get_variable('b',\n",
    "                           dtype=tf.float32,\n",
    "                           initializer=initial)\n",
    "\n",
    "with tf.name_scope('Input'):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, seq_max_len, input_dim], name='X')\n",
    "    y = tf.placeholder(tf.float32, shape=[None, out_dim], name='Y')\n",
    "    keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(x, num_hidden, out_dim, name, use_activation=True, keep_prob = None):\n",
    "    with tf.variable_scope(name):\n",
    "        # create weight matrix initialized randomely from N~(0, 0.01)\n",
    "        weights = weight_variable(shape=[num_hidden_units, out_dim])\n",
    "\n",
    "        # create bias vector initialized as zero\n",
    "        biases = bias_variable(shape=[out_dim])\n",
    "\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(num_hidden)\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)\n",
    "        out = tf.matmul(outputs[:, -1, :], weights) + biases\n",
    "        \n",
    "        layer = tf.nn.dropout(out, keep_prob)\n",
    "        \n",
    "        layer = tf.layers.dense(layer,units=out_dim)\n",
    "        \n",
    "        layer = tf.nn.softmax(layer)\n",
    "        \n",
    "    \n",
    "        return layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0703 03:22:34.027313 139693720700736 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "logits_out = LSTM(x, num_hidden_units, out_dim=out_dim, name = 'lstm1',use_activation=True, keep_prob = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y, logits_out))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss=3.9700379371643066\n",
      "Step 100, Loss=3.5526018142700195\n",
      "Step 200, Loss=3.1110730171203613\n",
      "Step 300, Loss=3.1790900230407715\n",
      "Step 400, Loss=3.2271814346313477\n",
      "Step 500, Loss=3.009063243865967\n",
      "Step 600, Loss=2.682675838470459\n",
      "Step 700, Loss=2.8985464572906494\n",
      "Step 800, Loss=2.739187717437744\n",
      "Step 900, Loss=2.526944637298584\n"
     ]
    }
   ],
   "source": [
    "# Creating the op for initializing all variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for i in range(training_steps):\n",
    "    x_batch, y_batch = next_batch(x_train, y_train, batch_size)\n",
    "    _, batch_loss = sess.run([train_op, loss], feed_dict={x: x_batch, y: y_batch})\n",
    "    if i % display_freq == 0:\n",
    "        print('Step {}, Loss={}'.format(i, batch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "import random\n",
    "\n",
    "def generate_text(length, diversity):\n",
    "    # Get random starting text\n",
    "    start_index = 0#random.randint(0, len(text) - maxlen - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    for i in range(length):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = sess.run(tf.squeeze(sess.run(logits_out, feed_dict = {x: x_pred})))\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2  - diversity----------------------------------------\n",
      "First Citizen:\n",
      "Before we proceed any furtathe ted  iin te te te ne wis ralle cirine ten on tite rite thito te  te be te ine to retill\n",
      "lIl\n",
      "\n",
      "Ficte wizen\n",
      "U it tes ibn ter hinnt te shor teal in tite bte te ise yon inre ar iter Ne tie se ciren ire tin in wet al tis to to es tare ante Fenllllt ant Ciinres y in ten\n",
      "Fon tes iine wen\n",
      "Fhi tenle te \n",
      "******************************************************\n",
      "0.5  - diversity----------------------------------------\n",
      "First Citizen:\n",
      "Before we proceed any fure ante. hesonribe tee . I oto vicin: eass thitt aan, ery itel he harat on iear Fas wito te ta, teel itenll hhitu wiandheer citilzt tar iiarnte u. hom cponf tae tou we ovs te ten ars\n",
      "Itlkhe cint iisnyitt\n",
      "I oon ret icenr taEva Nr the tiis rten bit  wete geoves anrlder ihztt et gk uhhe tato u ciner ChE\n",
      "******************************************************\n",
      "1.0  - diversity----------------------------------------\n",
      "First Citizen:\n",
      "Before we proceed any furuevhsrn apuven Tore\n",
      "Wilty hez seCeyuttU ha u rovaun uer hesde be cuw,s\n",
      "Fh llyf tyoe-s ts\n",
      "Fhol tTe iactyh ore\n",
      "FiaciMe-Shere hhse\n",
      "ioude lo mbtianmid ea  Ftea..\n",
      ":O' ro\n",
      "saSmotd Rifedcfabse ton eiYdel ry\n",
      "ChtpM'y\n",
      "-guann tactort 'ent, ganeme iinEFoirys\n",
      "gwhar soi tid aM !?U:lEy urgnrter Sarysile,e' eu ibs t\n",
      "******************************************************\n",
      "1.2  - diversity----------------------------------------\n",
      "First Citizen:\n",
      "Before we proceed any furpaar'l p raocne'o slo uoD.n Bitny\n",
      "'? :CiiFw,e:\n",
      "cHtme oeyrn yrllfpvyn -umfo tiDhilctet\n",
      "whe atiro. ; igtbns fodlnfoteei: CtaSs kg,\n",
      "bud;so. teor'! !eRzSFce 's:EOyL, jiulwhav - tw\n",
      "TSs!\n",
      "I\n",
      "Utol  afs-osctwod\n",
      ":mwgs td ntalhhrs ien ob TaCes Hykd,Lornslyto\n",
      "wwar opzinst tor Df ourno!RyThetySwet asyorcouver!, C\n",
      "******************************************************\n"
     ]
    }
   ],
   "source": [
    "for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "  print(diversity,\" - diversity----------------------------------------\")\n",
    "  print(generate_text(300, diversity))\n",
    "  print('******************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
