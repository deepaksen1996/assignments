{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('shakespere.txt', 'r').read()\n",
    "text = text[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars:  53\n",
      "number of sequences: 4900\n"
     ]
    }
   ],
   "source": [
    "# Getting sorted listof char in text\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars: ', len(chars))\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# Spitting text into sentences\n",
    "maxlen = 100\n",
    "step = 1\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('number of sequences:', len(sentences))\n",
    "\n",
    "# Character level One - hot Encoding\n",
    "x_train = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y_train = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_train[i, t, char_indices[char]] = 1\n",
    "    y_train[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Next Batch Generator\n",
    "def next_batch(x, y, batch_size):\n",
    "    N = x.shape[0]\n",
    "    batch_indices = np.random.permutation(N)[:batch_size]\n",
    "    x_batch = x[batch_indices]\n",
    "    y_batch = y[batch_indices]\n",
    "    return x_batch, y_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Dimensions\n",
    "input_dim = len(chars)       # input dimension\n",
    "seq_max_len = maxlen         # sequence maximum length\n",
    "out_dim = len(chars)         # output dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameters\n",
    "learning_rate = 0.01    # The optimization initial learning rate\n",
    "training_steps = 10000  # Total number of training steps\n",
    "batch_size = 64        # batch size\n",
    "display_freq = 1000    # Frequency of displaying the training results\n",
    "num_hidden_units = 64   # number of hidden units "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "    return tf.get_variable('W',\n",
    "                           dtype=tf.float32,\n",
    "                           shape=shape,\n",
    "                           initializer=initer)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n",
    "    return tf.get_variable('b',\n",
    "                           dtype=tf.float32,\n",
    "                           initializer=initial)\n",
    "\n",
    "with tf.name_scope('Input'):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, seq_max_len, input_dim], name='X')\n",
    "    y = tf.placeholder(tf.float32, shape=[None, out_dim], name='Y')\n",
    "    keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(x, num_hidden, out_dim, name, use_activation=True, keep_prob = None):\n",
    "    with tf.variable_scope(name):\n",
    "        # create weight matrix initialized randomely from N~(0, 0.01)\n",
    "        weights = weight_variable(shape=[num_hidden_units, out_dim])\n",
    "\n",
    "        # create bias vector initialized as zero\n",
    "        biases = bias_variable(shape=[out_dim])\n",
    "\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(num_hidden)\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)\n",
    "        out = tf.matmul(outputs[:, -1, :], weights) + biases\n",
    "        \n",
    "        layer = tf.nn.dropout(out, keep_prob)\n",
    "        \n",
    "        layer = tf.layers.dense(layer,units=out_dim)\n",
    "        \n",
    "        layer = tf.nn.softmax(layer)\n",
    "        \n",
    "    \n",
    "        return layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0702 22:48:50.244016 139693720700736 deprecation.py:323] From <ipython-input-9-84bf1a8503de>:9: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0702 22:48:50.247111 139693720700736 deprecation.py:323] From <ipython-input-9-84bf1a8503de>:10: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0702 22:48:51.680696 139693720700736 deprecation.py:506] From /home/deepak/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0702 22:48:51.689752 139693720700736 deprecation.py:506] From /home/deepak/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0702 22:48:51.997960 139693720700736 deprecation.py:506] From <ipython-input-9-84bf1a8503de>:13: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0702 22:48:51.998559 139693720700736 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0702 22:48:52.010332 139693720700736 deprecation.py:323] From <ipython-input-9-84bf1a8503de>:15: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "logits_out = LSTM(x, num_hidden_units, out_dim=out_dim, name = 'lstm1',use_activation=True, keep_prob = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y, logits_out))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss=3.9702374935150146\n",
      "Step 1000, Loss=2.8147709369659424\n",
      "Step 2000, Loss=2.409313678741455\n",
      "Step 3000, Loss=1.9547215700149536\n",
      "Step 4000, Loss=1.782707691192627\n",
      "Step 5000, Loss=2.298971652984619\n",
      "Step 6000, Loss=1.9245495796203613\n",
      "Step 7000, Loss=1.9863770008087158\n",
      "Step 8000, Loss=1.6021263599395752\n",
      "Step 9000, Loss=1.5493192672729492\n"
     ]
    }
   ],
   "source": [
    "# Creating the op for initializing all variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for i in range(training_steps):\n",
    "    x_batch, y_batch = next_batch(x_train, y_train, batch_size)\n",
    "    _, batch_loss = sess.run([train_op, loss], feed_dict={x: x_batch, y: y_batch})\n",
    "    if i % display_freq == 0:\n",
    "        print('Step {}, Loss={}'.format(i, batch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "import random\n",
    "\n",
    "def generate_text(length, diversity):\n",
    "    # Get random starting text\n",
    "    start_index = 0#random.randint(0, len(text) - maxlen - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    for i in range(length):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = sess.run(tf.squeeze(sess.run(logits_out, feed_dict = {x: x_pred})))\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deepak/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are ir rentaticalinae, the to cohanulteU rein\n",
      "nouenatrutr ther for th the nist.\n",
      "\n",
      "First Citizen:\n",
      "out atn thin asd yinseronow here tise heneth wirute theresel\n",
      " ontreut, renest in. Ast the nothth we ous bey to they bet usnetaned aroud caelysneli tolly on tholt to\n",
      "vide litt ther his lith to court, till bre nos urfn thor nithet\n",
      " uss artls, s el  hiv ne alt the worele to the Cilino talesh yeey storg he it cithen to shous belr us. Thie oun that walist faalry\n",
      "Wtath de sss an tol nater hit serthas enoss wo neos siren tored sontound wamy wo ci is lit to\n",
      "shof owe non to to colrtht us\n",
      " hore theye oned ir to.\n",
      "\n",
      "Aved: witths eres anterrigaceticory hiy tas om eslinces usw furt to de eus ther not theurenlod to noh thorer, worele the oen ansenek id fitrht us paunst thellics aede.\n",
      "\n",
      "Fetlt Citizen:\n",
      "Wirt he zenere urk, we ar sigohs sehalyeln but in thansmenes warhe cit to,\n",
      "Tht tudt anse ant hey nie fit were\n",
      "\n"
     ]
    }
   ],
   "source": [
    " print(generate_text(900, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
